<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio-Vision</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>

    <div class="colonne1">
        <!-- Contenu de la première colonne -->
        <div class="container">
            <!-- Colonne gauche fixe -->
            <div class="left-column">
                <div class="fixe">
                    <h1>Audio-<br>Vision</h1>
                    <div class="sommaire">
                        <h2>Sommaire<br><br><br><br></h2>
                        <ul>
                            <li><a href="#section1">1/ VidéoMusique, vers<br> une nouvelle expression <br>visuelle de la musique<br><br></a></li>
                            <li><a href="#section2">2/ Maîtrise totale <br>de l’image animée<br><br></a></li>
                            <li><a href="#section3">3/ Improvisation devenant <br>poésie audiovisuelle</a></li>
                            
                        </ul>
                    </div>

                    <div id="bouton-retour-container" style="display: none;">
                        <h2 id="bouton-retour" style="cursor: pointer; color: inherit; text-decoration: none;">Retour</h2>
                    </div>

                    <div class="bibliography">
                        <h2 id="bibliography-link">Bibliographie</h2>
                    </div>
                </div>
                
            </div>
        </div>
    </div>

    
    
    



    
    <div class="colonne2">
        <div id="definition-transduction" class="definition">
            1<br>Conversion d’un type de signal en un autre, souvent utilisé pour désigner le passage d’un média à un autre (ex. audio vers vidéo).
        </div>
        <div id="definition-sampling" class="definition">
            2<br>On traduit le terme sample par «échantillon». Créer un sample revient à capturer un extrait sonore existant puis le remanier - par exemple, accélérer ou ralentir son rythme, changer d’instrument pour le jouer, le faire monter en aigu ou baisser en grave - de manière à se l’approprier avant de l’incorporer dans une nouvelle musique.
        </div>
        <div id="definition-opensource" class="definition">
            3<br>L’Open Source est une méthode d’ingénierie logicielle qui consiste à développer un logiciel, ou des composants logiciels, et de laisser en libre accès le code source produit. Ce procédé peut être utilisé dans le but de développer une communauté afin de faire évoluer collectivement le logiciel original.
        </div>
        <div id="definition-java" class="definition">
            4<br>Le java est un langage de programmation qui permet de coder des logiciels web ou mobiles. Il est très souvent employé pour des projets d’animation numérique, par exemple sur le logiciel Processing.
        </div>
        <div id="definition-sérendipité" class="definition">
            5<br>Capacité, aptitude à faire par hasard une découverte inattendue et à en saisir&nbsp;l'utilité.
        </div>
        <div id="definition-midi" class="definition">
            6<br>En anglais Musical&nbsp;Instrument Digital Interface, permet l'échange d'instructions entre instruments et logiciels de musique électronique.
        </div>
        <div id="definition-ascii" class="definition">
            7<br>En anglais American Standard Code for Information Interchange, <br>est une norme informatique d'encodage de caractères qui répertorie tous les chiffres, lettres minuscules, capitales, des symboles mathématiques et de ponctuation. Il est possible avec des programmes de manipuler ces caractères comme des mosaïques afin de créer de l’image.
        </div>
        <div id="definition-texture" class="definition">
            <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>8<br>Une texture en 3D correspond à l’enveloppe qui va être associée à un volume, en quelque sorte sa peau. Un modèle 3D par défaut est gris, lui ajouter une texture lui permet d’avoir une surface différente.
        </div>
        <div id="definition-polygone" class="definition">
            <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>9<br>La définition d’un modèle 3D se définit par son nombre de polygones, un modèle 3D détaillé est appelé High Poly et à l’inverse Low Poly. 
        </div>
    </div>
    

    <div class="colonne3">
            <p>La relation entre son et l’image en mouvement soulève des questions sur les mécanismes permettant leur traduction directe. Par «traduction», il s’agirait d’un processus qui permettrait de passer du média «audio» vers un média «vidéo», sans qu’il n’y ait d’interférence entre un point A et un point B. On pourrait donc parler de transfert, de conversion ou de <span id="transduction">transduction¹</span>. Il est important d’établir une différence avec le terme d’«interprétation». En effet, interpréter revient à imaginer, donner à voir quelque chose de nouveau, de purement subjectif, en fonction d’un matériau préexistant. Une traduction, quant à elle, impose une équivalence directe, donc avec l’approche scientifique la plus objective possible. Il s’agirait donc d’une production audiovisuelle dans le sens le plus primitif du terme, une abstraction visuelle du son. Cette étude se concentrera sur le développement d’un processus de traduction numérique devient langage visuel au service d’un son.
            </p>
            <h2 id="section1">VidéoMusique, vers <br>une nouvelle expression visuelle de la musique</h2>
            <p>J’ai toujours été fasciné par la création de narrations
                 visuelles grâce à la musique, en particulier dans les clips vidéo. 
                 Des clips comme ceux de Gorillaz, Daft Punk ou Siames arrivent à créer des 
                 trames narratives et à délivrer des messages forts seulement grâce à un clip de musique.
                  Je me suis donc demandé quel était le processus d’un artiste pour passer du son à de l’image. Arriver à développer un univers graphique à partir de sons est un exercice auquel j’ai toujours voulu m’initier.
                J’ai déjà eu l’occasion de m’interroger sur la création de narration grâce à la musique dans ma pratique. Dans plusieurs de mes projets, j’interroge l’utilisation d’images fixes vers de l’animation sous plusieurs formes : de la linogravure, de l’édition (flipbook)
                <span class="black-square" data-image="images/flipbook.png,images/flipbook2.jpg" data-description="Décomposition de la photographie Cathedral of the pines de Gregory Crewdson "> </span>. Récemment, j’ai aussi pu travailler sur le
                


                <span id="sampling">sampling²</span>, à la fois sonore et visuel. Dans l’idée de traduction musique/image, j’ai repris l’onde de musique pour l’utiliser comme matière graphique.<span class="black-square" data-image="images/silouhettespectre.png" data-description="Sampling visuel des sons de la musique Let Mom Sleep de Hideki Naganuma"> </span>
                </p>

            <p>J’explique dans cette étude comment la maîtrise totale d’un processus créatif dans la création d’un langage peut ensuite mener à l’improvisation et proposer une nouvelle forme de production audiovisuelle nommée vidéomusique. Je ne préfère pas parler de clip vidéo pour plusieurs raisons : la notion générique de clip vidéo revêt des acceptions très larges, qui  renvoient à un support visuel d'un morceau de musique ou d'une chanson. La frontière entre clip vidéo et court métrage est très fine, même si dans les deux cas, il s’agit de productions audiovisuelles. Généralement, un clip est au service du son qu’il illustre, tandis que dans un court métrage, le visuel et le son sont sur le même piédestal. Les limites entre ces deux appellations restent assez floues. Personnellement, je trouve que le terme anglais «music video» est beaucoup plus révélateur et ne laisse pas de place à l'incertitude. Dans ma recherche de traduction musicale, le principe de «vidéo musicale» est plus pertinent que celle de clip, que l’on rapproche davantage à une vidéo à but commercial et à diffusion à grande échelle. Dans De la musique et des images
                (The Invisible Framework of Music and Images - Jean Piché, 2003), le compositeur et vidéaste Jean Piché propose d’ailleurs l'appellation «vidéomusique» :
                </p>
                <p class="blockquote"> 
                    «Ce que je propose est une extension du concept de vidéo clip vers une forme beaucoup plus ambitieuse qui se rapproche de celle du cinéma.
                     Une forme où le contenu musical dépasse en finesse et en subtilité ce que propose le vidéoclip de commerce. 
                     Une forme qui s’approprie tous les moyens de production numériques tant pour l’image que pour la musique. 
                     Une forme hybride qui livre en même temps en image et en musique une vision poétique et ouverte de l’imaginaire. 
                     Foncièrement, cet art hybride aura un fondement technologique. Il nourrira son expression par les artifices de l’image manipulée,
                      au même titre que la musique électroacoustique se justifie par les transformations du son. La vidéomusique se définira aussi par une absence :
                       celle d’un narratif déterminant puisqu’elle prétend à la poésie sensorielle. Si le cinéma est un roman audiovisuel,
                        la vidéomusique sera de la poésie audiovisuelle. L’ouverture de sa forme permettra l’exploration débridée du magique,
                         du fantasmagorique et d’un nouveau type d’absolu : la musique devenue image.»
                    </p>
            <p>Le terme de vidéomusique a été repris par le CNC (Centre national du Cinéma et de l’Image animée).</p>
            <p>Pour contextualiser, dès le début du 20e siècle, plusieurs artistes d’avant-garde ont voulu faire voir leur musique.
                 Le compositeur russe Alexandre Scriabine est le premier musicien qui a abordé ce questionnement dans une approche synesthète,
                  qui consistait à relier chaque touche de son clavier à des leds de couleurs. Ce système de «clavier à lumière»<span class="black-square" data-image="images/claviercouleurs.png" data-description="Le clavier à lumières Alexandre Scriabine"> </span> avait été conçu en associant couleurs et notes par le biais neurologique. Malheureusement, les technologies de l’époque n’ont pas permis de mener à bien ce projet. Ce système d’association d’images et de couleurs mentales a été repris par le plasticien-cinéaste Oskar Fischinger, qui est un des seuls plasticiens-cinéastes de l'avant-garde allemande des années 1920 à avoir axé sa carrière sur et autour de l'abstraction filmique musicale. Ses courts-métrages sont des stop-motions qui ont pour intention de représenter la musique visuellement grâce à des formes simples et abstraites en mouvement. 
            </p>
            <div class="colonne">
            
                <div class="youtube">
                    <iframe
                        src="https://www.youtube.com/embed/6Xc4g00FFLk?"
                        title="YouTube video player"
                        frameborder="0"
                        style="width: 25vw;height: 20vw;"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                        allowfullscreen>
                        
                    </iframe>
                </div>
            
            </div>
            
            <p>C’est en observant ce genre de productions audiovisuelles que l’on s'aperçoit que le vocabulaire descriptif appartenant au champ du visuel et à celui du son est très proche. En effet, un son peut tout à fait être spatialisé grâce au stéréo : on peut définir sa hauteur, sa saturation, composer des motifs… Produire une musique est en quelque sorte une «mise en page» de sons. 
                Ces caractéristiques du son appartiennent à un domaine technique, mais les producteurs peuvent également qualifier un son grâce à des émotions, des couleurs, des personnalités, des températures et même du goût. On peut citer les adjectifs «flou», «chaud», «métallique», «criard», «gras», «timide», «épicé», «sucré»... Cependant, ce vocabulaire fait appel à des métaphores auditives qui font appel à l’imagination, ce qui n’est pas forcément à la portée de tout le monde et peut paraître perturbant pour un public non connaisseur des processus de production musicale. Justement, c’est en proposant des productions audiovisuelles concrètes que ce langage peut devenir instinctif et donc libre à interprétation.
                </p>
            <p>Pour ma proposition de vidéomusique, j’aimerais développer un langage graphique basé sur la précision et l’improvisation. Pour développer ces concepts, j’ai réfléchi à plusieurs principes graphiques, outils, langages qui seraient pertinents pour ma proposition.</p>
            
            <h2 id="section2">Maîtrise totale <br>de l’image animée</h2>
            <p>
                Dans la recherche de précision, le code peut se révéler être un atout. Appelé «design génératif», il permet de directement travailler avec un langage pour produire de l’image : il n’y a pas de logiciel intermédiaire qui contraint à l’utilisation d’une sélection d’effets. L’utilisateur est libéré de toutes ces contraintes techniques ; la seule limite est la connaissance même du code. Il est possible de créer des animations complexes en seulement quelques lignes de code. Une suite de code est appelée un «script», c'est-à-dire les instructions données pour le rendu qui va être représenté dans la console. En comparaison avec les logiciels qui utilisent des systèmes de calque, en utilisant le code, tout est sur un même plan et il n’y a pas de réelle limitation de performance, car on travaille avec des formes géométriques simples, qui n’ont pas de résolution définie. Le coding peut sembler inaccessible à cause de sa complexité, c’est pourquoi il existe un outil 
                <span id="opensource">open source³</span>, nommé
                <a href="https://p5js.org/" class="dotted-link" target="_blank">P5JS</a>, qui permet d’apprendre facilement le javascript. <a href="https://p5js.org/" class="dotted-link" target="_blank">P5JS</a> a pour ambition de rendre l'esquisse de code aussi intuitive que celle dans un carnet de notes.
                C’est dans les années 1960, avec John Whitney, musicien et cinéaste expérimental, considéré aujourd’hui comme le père de la conception d'images assistée par ordinateur, que le code a été utilisé la première fois pour créer de l’image en mouvement de manière artistique. Il fait également partie des artistes qui voulaient faire voir la musique. Pour le court métrage Matrix III, il a repris une musique de Terry Riley, un des fondateurs de la musique minimaliste. À la manière d’un mandala en constante évolution, les animations, qui se composent et se décomposent sans cesse au son des improvisations de Terry Riley, forment une œuvre majeure en termes de synesthésie : la lumière, les formes et les sons se fondent, proposant une immersion hypnotique. En jouant sur l’accumulation de formes identiques, leurs mouvements et changements d'échelle peuvent créer des illusions de volumes. Encore aujourd’hui, où la technologie est bien plus développée, de nombreux artistes adhèrent à cette&nbsp;esthétique.
            </p>
            

                <div class="colonne">
                    <div class="vimeo">
                            <iframe
                                width="560"
                                height="315"
                                src="https://player.vimeo.com/video/1047257396?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" 
                                frameborder="0" 
                                allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media" 
                                title="John Whitney-Matrix III (1972)">
                            </iframe>
                        
                    </div>
                </div>
            <p>Divisional Articulations est l’un des courts métrages abstraits du réalisateur expérimental Max Hattler, sorti en 2017.
                 La musique a été réalisée par le compositeur contemporain Lux Prima. Le court métrage a été produit grâce à du code, sûrement du 
                 <span id="java">java⁴</span>.
                  Il met en scène des formes simples et neutres, qui donnent à voir un ensemble en grande partie monochrome. Ces formes géométriques,
                   rappelant le Bauhaus et le constructivisme russe, sont démultipliées, ce qui rend la composition très complexe et réfléchie.
                    Les mouvements sont très mécaniques, saccadés, en synchronisation parfaite avec la musique jouée. Chacun de ces mouvements laisse des traînées,
                     ce qui donne cet effet procédural de démultiplication, d'amoncellement. Aucune forme ne disparaît vraiment,
                      car elles sont superposées à la suite à une grande vitesse. On assiste à un vrai flux organique :
                       chaque forme semble bouger de manière calculée, afin qu’elle soit en accord avec la musique. Chaque mouvement/apparition
                        de forme a son utilité dans la composition globale. C’est comme si les formes créaient la musique au lieu de s’y adapter :
                         on observe une vraie machine en action. Le travail sur la musique est mis sur le même plan que celui sur le visuel.
                          Chaque élément est influencé par des sons électriques, ce qui vient les accélérer, les ralentir, les tordre… 
                          À mesure que des sons sont ajoutés à la composition sonore, la composition visuelle se complexifie également,
                           créant un paysage épileptique et hypnotisant. L'œil se balade, tenté de suivre une forme pour savoir ce qu’elle va devenir
                            et comment elle va contribuer à l’ensemble. À chaque visionnage du court métrage, on peut assister à une nouvelle expérience,
                             remarquer de nouveaux motifs, types de formes. Les mélanges de formes font directement appel à l’imagination des spectateurs :
                              chaque élément est objectivement en 2D, mais il peut être perçu en volume, une fois relié à d’autres. 
                              Les flux sont entre improvisation et&nbsp;ultime&nbsp;précision.</p>
                              
                            <div class="colonne">
                                <div class="vimeo">
                                    <iframe
                                        width="560"
                                        height="315"
                                        src="https://player.vimeo.com/video/218815513?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                                        frameborder="0"
                                        style="margin-top: -3vw;margin-bottom:-3vw"
                                        allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                                        title="Divisional Articulations (4:33)">
                                    </iframe>
                                </div>
                            </div>
                            

                <p>J’ai déjà essayé d'expérimenter la création d’un langage en utilisant des formes simples pour représenter la musique Vernacular de l’artiste Zelezna.
                 J’ai choisi cette musique, car l’artiste voulait lui-même développer un langage musical. Le morceau Vernacular, qui commence avec de nombreux éléments rythmiques,
                  a été pensé comme une tentative de construction de langage, comme un petit alphabet qui prendrait forme.
                   Il m’a suffi de seulement trois formes différentes<span class="black-square" data-image="images/formes.png"></span> pour mettre en images les trois sons électroniques utilisés dans la musique.</p>

                   <div class="colonne">
                    <div class="vimeo">
                        <iframe
                            width="560"
                            height="315"
                            src="https://player.vimeo.com/video/1047932967?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                            frameborder="0"
                            style="margin-bottom: 3vw; margin-top: 1vw;"
                            allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                            title="Vernacular">
                        </iframe>
                    </div>
                </div>
                

                    <p>L’animation 3D peut également être une réponse à mon questionnement. L’utilisation du logiciel Blender est complètement gratuite et open-source, 
                        ce qui permet l’entière modification du logiciel grâce à des plug in proposés par son importante communauté. 
                        À la manière du coding, les formes géométriques brutes prennent cette fois un espace en trois dimensions, 
                        ce qui permet de proposer des productions plus immersives. Par rapport à de la 2D, 
                        la retranscription d’une musique dans un environnement 3D met en lumière de nouveaux enjeux : 
                        ajouter une nouvelle dimension à un espace apporte la profondeur (l’axe z) et les mouvements de caméra.
                         Par exemple, le clip No brain d'Etienne de Crécy, réalisé par le studio de design graphique H5,
                          propose un zoom constant dans un tunnel de cubes en mouvements synchronisés, à la manière d’un kaléidoscope.
                           En termes de moyens utilisés pour l’animation, 
                           les matériaux sont très simples et n’utilisent pratiquement que des cubes avec une sélection très réduite de couleurs.
                            La démultiplication, les mouvements, rotations rendent le tout très complexe et maîtrisé.
                    </p>

                    <div class="youtube">
                        <iframe
                            src="https://www.youtube.com/embed/Z-3z3DNUGiE?si=bb416pgVZ_klv5Sh"
                            title="YouTube video player"
                            frameborder="0"
                            style="width: 25vw;height: 20vw;"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen>
                            
                        </iframe>
                    </div>
                    <h2 id="section3">Improvisation devenant <br>poésie audiovisuelle</h2>
                <p>À l’intérieur même de cette maîtrise totale que propose le design génératif, il est même possible de contrôler l’aléatoire grâce à des variables. Un script n’est pas comme un fichier vidéo qui aura toujours le même résultat à chaque lancement. En effet, il est possible de créer des événements où c’est la machine qui va décider ce qui va se produire. On peut parler d’improvisation contrôlée, 
                une <span id="sérendipité">sérendipité⁵</span>. En effet, la question de l’aléatoire peut aussi déclencher des «erreurs»,
                 des lignes de codes qui entrent en conflit, ce qui peut donc créer de l’inattendu et avoir un intérêt, 
                 car cela engendre de la surprise. Si la production visuelle se contente de réagir sagement en rythme à l’entièreté des sons produits, 
                 le rendu sera forcément très prévisible. Il faut donc trouver un équilibre entre synchronisation et point de rupture. 
                 Par rapport aux possibilités d’improvisation, les performances live permettent de mettre en action un langage déjà réfléchi. 
                 Par exemple, la graphiste plasticienne SOIA a développé un répertoire de formes pour retranscrire la musique In C de Terry Riley ;
                  tout un langage a été développé au préalable et la performance amène l’improvisation.
                   On peut penser que l’improvisation va perturber le phénomène de traduction que je recherche, mais en réalité, 
                   un langage en soi est un matériau «fixe» composé de signes qui, une fois utilisé, peut s’accorder des libertés et se composer de manière spontanée,
                    tout en gardant une limite de compréhension.</p>
                    
                    <div class="youtube">
                        <iframe
                            src="https://www.youtube.com/embed/3AdKcD6lFh4?"
                            title="YouTube video player"
                            frameborder="0"
                            style="width: 25vw;height: 20vw;"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen>
                            
                        </iframe>
                    </div>

                    <p>Les deux graphistes du studio Superscript2 codent entièrement leurs propres outils
                     de création graphique et ils travaillent la plupart du temps dans l’univers de la musique. 
                     Ils ont notamment développé un programme pour DJ qui relie directement l’image à la table de mixage grâce à une alimentation en <span id="midi">midi⁶</span> ;
                      il y a donc là une création simultanée de l’image et du son. Encore une fois, le langage visuel est pensé à posteriori et c’est au DJ
                       d’expérimenter avec l’outil. C’est une production modulable qui dépend de la manière dont l’utilisateur va s’en servir.</p>
                
                       <div class="colonne">
                        <div class="vimeo">
                            <iframe
                                width="560"
                                height="315"
                                src="https://player.vimeo.com/video/1037391453?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                                frameborder="0"
                                style="margin-bottom: 3vw;margin-top: 1vw;"
                                allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                                title="DJ set Superscript">
                            </iframe>
                        </div>
                    <p>Pour parvenir à traduire un son visuellement, de la manière la plus objective possible,
                         il est primordial d’avoir recours à des principes caractérisés comme scientifiques ou à des pratiques réglées.
                          Il est vrai qu’un signal sonore est une onde et une onde est déjà une image.
                           En effet, une onde sonore est représentée visuellement grâce à des systèmes appelés «spectrogrammes»  et «sonagrammes»<span class="black-square" data-image="images/spectre.png"></span>,
                            qui donnent la puissance et la fréquence d’un son. Cependant, il serait trop basique de les présenter tels quels
                             dans une production concrète. Ce sont des langages déjà préétablis et l’enjeu du projet est justement d'en proposer un nouveau.
                              Il est néanmoins possible d'opérer une réinterprétation de ces langages, un détournement. 
                        Il existe une machine appelée «oscilloscope»<span class="black-square" data-image="images/oscilloscope.jpg"></span> qui permet de traduire un son en onde sur un écran grâce aux canaux stéréo : le canal gauche correspond aux ondes verticales et le canal droit aux ondes horizontales. On observe donc une double création simultanée du son et de l’image. L’artiste Jerobeam Fenderson a notamment réalisé un album entier en se basant sur cette machine. Il a donc à la fois travaillé le son, mais aussi le visuel qui est engendré. Par exemple, il est possible d'utiliser Blender avec un plugin pour transférer une animation 3d en signal sonore utilisable sur un oscilloscope. Ce système permet d'implémenter directement un «clip» prêt à être visionné grâce à la composition musicale, ce qui retire toute dépendance à une plateforme de diffusion. Le musicien devient alors maître de sa musique et du visuel qu’il produit. Les formes que produisent ces machines peuvent être récupérées et réinterprétées pour la potentielle création de nouveaux langages.</p>
                    </div>

                    <div class="youtube">
                        <iframe
                            src="https://www.youtube.com/embed/kPUdhm2VE-o?si=w68CSBSjZQFl_xrG"
                            title="YouTube video player"
                            frameborder="0"
                            style="width: 25vw;height: 20vw;"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen>
                            
                        </iframe>
                    
                    </div>

                    <p>Par rapport à la reprise de matériaux graphiques, le studio Superscript2 s’est approprié l’aspect d’onde des spectrogrammes
                         pour créer des affiches animées pour un festival de musique électronique. Ces ondes ont été créées à partir de photographies
                          d’archive des éditions précédentes du festival<span class="black-square" data-image="images/affiche.jpg"></span>. Ces images ont été compressées, distordues, afin de perdre leurs sens initiaux. 
                    </p>
                    
                    <style>
                        .videos-cote-a-cote {
                            display: flex;
                            flex-wrap: wrap; /* Permet de passer sur plusieurs lignes si l'espace est insuffisant */
                            gap: 20px; /* Espace entre les vidéos */
                            justify-content: center; /* Centrer les vidéos horizontalement */
                        }
                        .videos-cote-a-cote .vimeo {
                            flex: 1 1 calc(50% - 20px); /* Chaque vidéo occupe 50% de la largeur moins l'espace entre les vidéos */
                            max-width: 800px; /* Largeur maximale des vidéos */
                        }
                        .videos-cote-a-cote iframe {
                            width: 100%; /* Adapter la vidéo à la largeur de son conteneur parent */
                            height: 450px; /* Définir une hauteur fixe correspondant à un ratio 16:9 */
                        }
                    </style>
                    
                    <div class="videos-cote-a-cote">
                        <div class="vimeo">
                            <iframe
                                src="https://player.vimeo.com/video/1047541706?h=83f0dd649e&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                                frameborder="0"
                                allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                                title="Affiche animée Nuits Sonores 2023 - Superscript">
                            </iframe>
                        </div>
                        <div class="vimeo">
                            <iframe
                                src="https://player.vimeo.com/video/1047544125?h=3a3dbcb032&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                                frameborder="0"
                                style="margin-bottom: 3vw;margin-top: 3vw;"
                                allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                                title="Affiche animée Nuits Sonores 2023 - Superscript (fond)">
                            </iframe>
                        </div>
                    </div>
                    <script src="https://player.vimeo.com/api/player.js"></script>
                    
                    <p>Le clip d’Aphex, Twin T69 Collapse, réalisé par Weirdcore, est aussi un exemple pertinent.
                         Tous les éléments introduits dans ce clip sont réappropriés et utilisés comme matière graphique.
                        Le code est directement utilisé comme matériau graphique, à la manière du <span id="ascii">ASCII⁷</span>.
                         On repère l’utilisation de photographies et de <span id="texture">textures⁸</span> de modèles 3D de bâtiments scannés.
                          Il y a un jeu entre distinction et destruction de ces images. Le décor est en constant changement ;
                           les modèles 3D s’étirent ; les textures qui les composent suivent donc le mouvement et perdent leurs sens initiaux. 
                           Au tempo très instable de la musique, des images apparaissent et disparaissent ;
                            elles ne sont utilisées que pour appuyer la transcription du rythme et deviennent presque subliminales.
                             L’accumulation de toutes ces images crée un bruit ambiant qui évolue vers un chaos total,
                              où plus rien de figuratif n’est perceptible. L’artiste Weirdcore remet en question «l’erreur» en animation 3D,
                               la corruption des visuels, la «mauvaise» réaction des textures associées aux modèles 3D qui perdent des <span id="polygone">polygones⁹</span>
                                ou changent totalement de&nbsp;formes. 
                        </p>
                    
                        <div class="youtube">
                            <iframe
                                src="https://www.youtube.com/embed/SqayDnQ2wmw?si=ZjQimpkGVNwQQusg"
                                title="YouTube video player"
                                frameborder="0"
                                style="width: 25vw;height: 20vw;"
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                allowfullscreen>
                                
                            </iframe>
                        </div>

                        <p>En informatique, une erreur visuelle s’appelle un «glitch» : une distorsion, déstructuration de l’image, se déclenche suite à une corruption interne des données numériques qui définissent l’image. L’artiste Rosa Menkman interroge ce phénomène : selon elle, le glitch ne se résume pas à un simple effet visuel. Elle questionne donc tous les processus qui en découlent et déclare : «le glitch est l’expérience magnifique d’une interruption qui détourne un objet de sa forme et de son discours ordinaire». Elle brise les barrières de la standardisation, de la résolution d’image, ce qui permet une grande liberté dans la production de visuels grâce aux outils numériques. 
                        </p>

                        <div class="colonne">
                            <div class="vimeo">
                                <iframe
                                    width="560"
                                    height="315"
                                    src="https://player.vimeo.com/video/501518614?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479"
                                    frameborder="0"
                                    style="margin-bottom: -3vw;margin-top: -3vw;"
                                    allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media"
                                    title="Stedelijk Museum Vernacular of File Formats installation video">
                                </iframe>
                            </div>
                        </div>

                        <p>Le sampling est un collage musical, un détournement d'un extrait d'une musique, il peut être intéressant de revenir aux origines du son pour se rendre compte s'il a été étiré, accéléré, rendu plus ou moins aigu etc. Ces modifications appliquées au son peuvent être reprises dans le champ du design. Ce processus est similaire au rapprochement instinctif du vocabulaire descriptif sonore et visuel évoqué dans la première partie. Ce procédé va plus loin que le sampling, c’est une attitude à adopter pour observer le processus créatif du producteur de musique, un mouvement, l’enregistrement d’un bruit peut être déterminant dans l’extraction de principes graphiques.
                        </p>

                        <p>Le collaborateur avec lequel je vais travailler s'appelle Stanlee Harris, il compte faire un album de musique électro-acoustique. Ce genre musical est au cœur même de ma recherche, déjà qualifié d’expérimental, remet en question l'utilisation de sons créés ou traités numériquement, couplés avec des sons provenant d’instruments acoustiques. Il s'agit d'un genre musical très proche de la musique concrète, qui considère de la même manière les sons harmoniques et les bruits. Il y a une vraie morphose entre des sons composés de manière complètement différente : un son purement électronique n'a pas du tout la même consistance visuelle que le son enregistré d'un instrument. Un son acoustique apporte la question de l'espace dans lequel il a été enregistré et avec quel moyen il l’a été. À la manière d'un musicien qui voudrait associer des sons provenant d'un univers numérique à des sons acoustiques, j’ai l'intention de proposer à mon tour une combinaison de deux médias distincts, le son et le visuel.
                            Cette hybridation d’outils et de techniques que je propose permet l’émancipation de toute règle mathématique. Déstabiliser des normes en les faisant s’entrechoquer amène à créer ce phénomène que l’on appelle Vidéomusique. Comme vu précédemment, le compositeur Jean Piché décrit la vidéomusique comme de la «poésie audiovisuelle» et «sensorielle», où les sonorités et rythmes sont amenés à être perçus grâce à un langage. Cette synesthésie se détache de toute forme de narration et laisse place à l’entière abstraction de l’image afin de s’ouvrir à l’imaginaire. Le stable et l’instable peuvent tout à fait cohabiter dans une seule et même production et c’est en créant de l’improvisation que l’on peut trouver de la subjectivité à l’intérieur de l’objectivité. 
                            </p>
                        


    </div>
    
    <div class="image-tooltip" id="image-tooltip">
        <img id="tooltip-image" src="" alt="Tooltip image">
    </div>




    <script src="animation.js"></script>
    
</body>
</html>
